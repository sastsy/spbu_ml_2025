{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HERrGPm5IJBT"
   },
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SmWz4pRoYww"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes, fetch_openml,load_iris,fetch_california_housing\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression, RFE, SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "RepeatedStratifiedKFold, \n",
    "cross_val_score, \n",
    "train_test_split, \n",
    "GridSearchCV,\n",
    "cross_val_predict, \n",
    "learning_curve, \n",
    "validation_curve)\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error,zero_one_loss, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from os.path import join as pjoin\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "#sharper plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV,\n",
    "                                  SGDClassifier)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/data/ml\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Tbl_ahIEfz"
   },
   "source": [
    "## Проклятие размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkemB_fZp687"
   },
   "source": [
    " Проклятие размерности означает, что по мере увеличения количества объектов или измерений в наборе данных объем данных, необходимый для эффективного моделирования взаимосвязи между объектами и целевой переменной, растет экспоненциально.\n",
    "Чем больше мы добавляем признаков, не увеличивая объем данных, используемых для обучения модели, тем больше увеличивается среднее расстояние между точками в пространстве признаков. Из-за такой разреженности становится гораздо проще найти удобное и совершенное, но не столь оптимальное решение для модели машинного обучения. Следовательно, модель плохо обобщает, что делает прогнозы ненадежными. Это может привести к таким проблемам, как переобучение и снижение точности.\n",
    "\n",
    " Чтобы смягчить проклятие размерности, можно использовать такие методы, как выбор признаков, уменьшение размерности и ансамблевые методы.\n",
    "\n",
    " Для примера рассмотрим 1000 случайных точек в пространствах размерности от 2 до 50. Построим график, показывающий это свойство."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "FhEx6m0spT00",
    "outputId": "92846944-c155-4504-8906-f327944ddaa7"
   },
   "outputs": [],
   "source": [
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = # YOUR CODE : generate random point\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mxd = max(diffs)\n",
    "    mnd = min(diffs)\n",
    "    delta = math.log10(mxd-mnd)/mnd # this shows the tightness of points region\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Width of points region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "D_N9K_WesZHu",
    "outputId": "5ebeb34d-998c-4939-95b5-06846152006f"
   },
   "outputs": [],
   "source": [
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = np.random.randint(-100,100,N)\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mnd = min(diffs)\n",
    "    delta = # YOUR CODE: plot logarithm of mean distance\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Mean Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9rrPXJftpWe"
   },
   "source": [
    "Для того, чтобы уменьшить влияние проклятия размерности, используются разные техники:\n",
    " - Feature selection. Выбор наиболее релевантных функций из набора данных может помочь уменьшить размерность и повысить производительность модели.\n",
    " - Снижение размерности. Такие методы, как анализ главных компонент (PCA), линейный дискриминантный анализ (LDA) и t-SNE, могут использоваться для уменьшения размерности данных при сохранении наиболее важной информации.\n",
    " - Регуляризация. Методы регуляризации, такие как L1 и L2, могут помочь предотвратить переобучение в многомерных наборах данных путем добавления штрафного члена к функции потерь.\n",
    " - Ансамблевые методы. Объединение результатов нескольких моделей также может помочь повысить производительность в многомерных наборах данных.\n",
    " - Увеличение размера набора обучающих данных также может помочь преодолеть проклятие размерности, однако сбор большего количества данных может занять много времени и стоит дорого.\n",
    " - Генерация синтетических данных. Генерацию синтетических данных также можно использовать для увеличения размера набора данных, особенно когда сбор большего количества реальных данных невозможен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJErrOewvGZ-"
   },
   "source": [
    "Рассмотрим несколько вариантов решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48cOIxYWo9cD"
   },
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI MAGIC dataset: https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope\n",
    "\n",
    "MAGIC (Major Atmospheric Gamma Imaging Cherenkov) - это система, состоящая из двух черенковских телескопов диаметром 17 м. Они предназначены для наблюдения гамма-лучей от галактических и внегалактических источников в диапазоне очень высоких энергий (от 30 ГэВ до 100 ТэВ).\n",
    "\n",
    "Телескопами MAGIC в настоящее время управляют около 165 астрофизиков из 24 организаций и консорциумов из 12 стран. MAGIC позволил открыть и исследовать новые классы источников гамма-излучения, таких как, например, пульсары и гамма-всплески (GRB).\n",
    "\n",
    "В наборе данных хранятся параметры зарегистрированных \"фотографий\" излучений потоков частиц от этих телескопов. Они делятся на  кванты (фотоны) и адроны (протоны). \n",
    "Описание признаков:\n",
    "\n",
    "- Length: continuous # major axis of ellipse [mm]\n",
    "- Width: continuous # minor axis of ellipse [mm]\n",
    "- Size: continuous # 10-log of sum of content of all pixels [in #phot]\n",
    "- Conc: continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
    "- Conc1: continuous # ratio of highest pixel over fSize [ratio]\n",
    "- Asym: continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- M3Long: continuous # 3rd root of third moment along major axis [mm]\n",
    "- M3Trans: continuous # 3rd root of third moment along minor axis [mm]\n",
    "- Alpha: continuous # angle of major axis with vector to origin [deg]\n",
    "- Dist: continuous # distance from origin to center of ellipse [mm]\n",
    "- Label: g,h # gamma (signal), hadron (background)\n",
    "\n",
    "g = gamma (signal): 12332 \n",
    "\n",
    "h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuCCBb0DpWuz"
   },
   "outputs": [],
   "source": [
    "columns = np.array([\"Length\", \"Width\", \"Size\", \"Conc\", \"Conc1\", \"Asym\", \"M3Long\", \"M3Trans\", \"Alpha\", \"Dist\"])\n",
    "\n",
    "data = pd.read_csv(pjoin(data_path, \"magic.txt\"),header=None, names=list(columns)+[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[columns].values\n",
    "y = 1 * (data['Label'].values == \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Проведите краткий EDA, определите, как выглядят признаки, есть ли пропуски итд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5V5bi9Dwkba"
   },
   "source": [
    "Самый простой способ выбора признаков - Information gain (прирост информации). Прирост информации — это статистика, которая измеряет снижение энтропии (неопределенности) для конкретной функции (таргета) путем деления данных в соответствии с этой характеристикой. Формально, он определен с помощью взаимной информации (I(X , Y) = H(X) – H(X | Y)). Она часто используется в алгоритмах дерева решений, а также имеет другие полезные свойства. Чем выше прирост информации от признака использования признака, тем полезнее он для принятия решений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFR8Pn3swh6I",
    "outputId": "fb9b4d5a-089f-4f4a-d152-15c3efd3af9e"
   },
   "outputs": [],
   "source": [
    "# Apply Information Gain\n",
    "ig = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "# Create a dictionary of feature importance scores\n",
    "feature_scores = {}\n",
    "for i, col in enumerate(list(columns)):\n",
    "    feature_scores[col ] = ig[i]\n",
    "\n",
    "# Sort the features by importance score in descending order\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importance scores and the sorted features\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"Feature: {feature}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(f_imps, f_names, title=\"\"):\n",
    "    f_imps = np.array(f_imps)\n",
    "    f_names = np.array(f_names)\n",
    "    sort_inds = np.argsort(f_imps)\n",
    "    yy = np.arange(len(f_imps)).astype(int)\n",
    "    plt.barh(yy, f_imps[sort_inds])\n",
    "    plt.yticks(yy, f_names[sort_inds], size=14)\n",
    "    plt.xticks(size=14)\n",
    "    plt.xlabel(\"Feature importance\", size=14)\n",
    "    plt.title(title, size=14)\n",
    "        \n",
    "    # Add importance scores as labels on the horizontal bar chart\n",
    "    for i, v in enumerate(f_imps[sort_inds]):\n",
    "        plt.text(v + 0.01, i, str(round(v, 3)), color=\"black\", fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(list(feature_scores.values()), list(feature_scores.keys()), title=\"Feature Importance Scores (Information Gain)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: какие из этих признаков можно считать важными? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2xvEMs014Y5"
   },
   "source": [
    "Другой вариант - F test. F-Test выполняет проверку гипотез по моделям X и Y, где X — это модель, созданная только с помощью константы, а Y — это модель, созданная с помощью константы и функции.\n",
    "\n",
    "Ошибки в обеих моделях сравниваются и проверяется, являются ли различия в ошибках между моделями X и Y значительными или они внесены случайно:\n",
    "\n",
    "Для каждого признака ошибка\n",
    "$$E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))$$\n",
    "преобразуется в F-статистику, после чего рассчитывается уровень значимости признака.\n",
    "\n",
    "Есть некоторые недостатки использования F-Test для выбора функций. F-Test проверяет и фиксирует только линейные связи между признаками и таргетами. Высококоррелированному признаку присваивается более высокий балл, а менее коррелированному признаку — более низкий балл. Однако даже  сильные нелинейные связи этот тест не выявит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tmnUkqDyMl8",
    "outputId": "ac984ddd-0ef6-475d-d354-079284d019c7"
   },
   "outputs": [],
   "source": [
    "f_statistic, p_values = f_classif(X_train, y_train)\n",
    "feature_scores = {}\n",
    "feature_scores = {}\n",
    "for i, col in enumerate(list(columns)):\n",
    "    feature_scores[col] = (f_statistic[i], p_values[i])\n",
    "\n",
    "# Sort the features by importance score in descending order\n",
    "sorted_features = sorted(\n",
    "    feature_scores.items(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "# Print the feature importance scores and the sorted features\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"Feature: {feature}, Score: {score[0]}, p-value: {score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: Какие из столбцов можно считать важными, основываясь на этом тесте? Поясните свой ответ.\n",
    "\n",
    "**Задание**: \n",
    "1) Постройте модель логистической регрессии, используя все признаки из набора.\n",
    "2) Также постройте модели для вабранных наборов признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsQPgQHVBEZQ"
   },
   "outputs": [],
   "source": [
    "def print_feature_names(features, original_feature_names: list  | None = None):\n",
    " if features is not None:\n",
    "  feature_names = [original_feature_names[int(f.strip(\"pixel\"))] for f in features]\n",
    "  print(f\"Selected features: {feature_names}\")\n",
    " else:\n",
    "  print(f\"Selected features: {fs.get_feature_names_out()}\")\n",
    "\n",
    "def select_features(X_train, y_train, X_test, function, k: int = 5, features: list  | None = None):\n",
    " # configure to select a subset of features\n",
    " fs = SelectKBest(score_func=function, k=k)\n",
    " fs.fit(X_train, y_train)\n",
    " # YOUR CODE: print feature names. Hint: Use the method of fs, which saves out features\n",
    " X_train_fs = fs.transform(X_train)\n",
    " X_test_fs = fs.transform(X_test)\n",
    " return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5_ynFIjAVOT",
    "outputId": "fef38992-1ead-49af-e8da-0969f93e717b"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "cross_val_score(\n",
    "    model, X, y, scoring=\"accuracy\", cv=5\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "qz5hwxNrBafl",
    "outputId": "29407dff-b769-43fb-bd08-1d16c550644d"
   },
   "outputs": [],
   "source": [
    "results = {\"num_features\": [], \"acc\": [], \"roc_auc\": []}\n",
    "for i in range(1, len(columns)):\n",
    "    results[\"num_features\"].append(i)\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs, fs = select_features(\n",
    "        X_train, y_train, X_test, f_classif, k=i, features = list(columns)\n",
    "        )\n",
    "    # fit the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_fs, y_train)\n",
    "    yhat = model.predict(X_test_fs)\n",
    "    probs = model.predict_proba(X_test_fs)[:, 1]\n",
    "    # evaluate predictions\n",
    "    results[\"acc\"].append(accuracy_score(y_test, yhat))\n",
    "    results[\"roc_auc\"].append(roc_auc_score(y_test, probs))\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем также оценить признаки, используя сам классификатор. В случае с деревьями, лесами и бустингами это очень естественно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $T(f)$ все ноды, использующие фичу $f$ для разбиения. Tогда, важность фичи $Imp(f)$ of $f$:\n",
    "    $$Imp(f) = \\sum_{t \\in T(f)} n_t \\Delta I(t),$$\n",
    "    $$\\Delta I(t) = I(t) - \\sum_{c \\in children} \\frac{n_c}{n_t} I(c),$$\n",
    "где $n_{t}$ - число объектов в ноде $t$; $I(t)$ – impurity function (gini, cross-entropy, MSE) в ноде $t$\n",
    "  \n",
    "В случае ансамбля важности фичей усредняются между деревьями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "plot_feature_importances(model.feature_importances_, columns, title=\"Feature Importance Scores (Information Gain)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае линейной модели, важность признака тоже можно оценить. В случае отнормализованных признаков, оценкой важности выступает модель соответствующего коэффициента.\n",
    "\n",
    "**Задание**: Оцените важность признаков, используя линейную модель. \n",
    "Подсказка: используйте l2 регуляризацию и <model>.coef_[0] для получения коэффициентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_imp_with_lin_mod(X_train, y_train, C=1.0):\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train_ss = ss.transform(X_train)\n",
    "    ### YOUR CODE\n",
    "    #\n",
    "    ### YOUR CODE\n",
    "    return f_imps_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, для разных моделей оценки сильно отличаются. \n",
    "Общий метод оценки важности: \n",
    "1) Обучите свою модель\n",
    "2) Рассчитайте метрику качества  $Q_o$  на тестовом наборе\n",
    "3) Для признака $f$:\n",
    "     1) Замените значения случайными значениями из того же распределения (выполните случайное перемешивание)\n",
    "     2) Рассчитайте метрику качества $Q_r$ на тестовом наборе \n",
    "     3) Оцените важность признака: $Imp(f) = Q_0 - Q_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Реализуйте такой метод, используя линейную модель с регуляризацией.\n",
    "\n",
    "**Бонусное задание**: Сделайте метрику аргументом, проведите тесты с линейной моделью, деревом решений, случайным лесом, двумя метриками. Постройте таблицу со сравнением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_imp_general(X_test, y_test, model):\n",
    "    feature_importances = []\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    q_0 = accuracy_score(y_test, y_test_proba)\n",
    "    for i in range(X_test.shape[1]):\n",
    "\n",
    "        # do not forget to make a copy of X_test!\n",
    "        X_test_copy = X_test.copy()\n",
    "\n",
    "        # shuffle values of the i-th feature\n",
    "        ### YOUR CODE\n",
    "        #\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        X_test_copy = np.nan_to_num(X_test_copy)\n",
    "        y_hat = model.predict(X_test_copy)\n",
    "        q_r = accuracy_score(y_test, y_hat)\n",
    "\n",
    "        importance = None # !! your code (calculate importance according to algorithm)\n",
    "        feature_importances.append(importance)\n",
    "\n",
    "    return np.array(f_imps_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJjvJyVi9NaF"
   },
   "source": [
    "Как же перейти от простой оценки важности признаков к поиску такого подмножества, которое приведет к наибольшей точности модели?\n",
    "Можно использовать рекурсиввный выбор признаков (Recursive feature selection, или RFE).\n",
    "RFE работает путем поиска подмножества признаков, начиная со всех признаков в наборе и постепенно удаляя самые незначимые, пока не останется желаемое количество.\n",
    "\n",
    "Это достигается путем подбора модели для оценки, ранжирования признаков по важности, отбрасывания наименее важного признака и повторного обучения модели. Этот процесс повторяется до тех пор, пока не останется заданное количество признаков. Характеристики оцениваются либо с использованием предоставленной модели, либо с использованием статистического метода, такого как f-score или IG.\n",
    "\n",
    "В sklearn реализован класс трансформера для такого подбора, при этом подюор может быть отделен от основной модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5cJqvjZ6nYu"
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "model = LogisticRegression()\n",
    "#pipeline = Pipeline(steps=[('sc',scaler),('s',rfe),('m',model)])\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "print_feature_names(rfe.get_feature_names_out(), columns)\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Реализуйте данный метод вручную. Бонусное задание: реализуйте класс трансформера для подбора и/или конфигурируемые метрику и модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_features_set(X_train, y_train, X_test, y_test, model, columns):\n",
    "    \"\"\"\n",
    "    Recursive feature elimination\n",
    "    \"\"\"\n",
    "    X_train_curr = X_train.copy()\n",
    "    X_test_curr  = X_test.copy()\n",
    "    f_names_curr = columns.copy()\n",
    "\n",
    "    # define a list for the feature importances\n",
    "    scores = []\n",
    "    # for each feature in the sample estimate its importance\n",
    "    for i in range(X_test.shape[1]):\n",
    "        # fit the model using current set of festures\n",
    "        # get feature importances\n",
    "        ### YOUR CODE\n",
    "        #\n",
    "        # importances = ...\n",
    "        ### YOUR CODE\n",
    "        yhat = model.predict(X_test_curr)\n",
    "        score = accuracy_score(y_test, y_hat)\n",
    "        print(\"Score: \", np.round(score, 4))\n",
    "        scores.append(score)\n",
    "        # remove feature with the least importance\n",
    "        min_importance = importances.min()\n",
    "        X_train_curr = X_train_curr[:, importances > min_importance]\n",
    "        X_test_curr  = X_test_curr[:, importances > min_importance]\n",
    "        f_names_curr = f_names_curr[importances > min_importance]\n",
    "    return scores, f_names_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, f_names  = find_best_features_set(X_train, y_train, X_test, y_test, model, columns)\n",
    "nf = np.arange(1, len(scores)+1)[::-1]\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(nf, scores, linewidth=3)\n",
    "plt.scatter(nf, scores, linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Number of features\", size=16)\n",
    "plt.ylabel(\"Score\", size=16)\n",
    "plt.xticks(size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EsjypD0omIj"
   },
   "source": [
    "### PCA\n",
    "\n",
    "[Ноутбук Евгения Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2022-spring/seminars/sem14_pca_tsne.ipynb)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и в некоторых ситуациях такой подход практически бесполезен. Например, если мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о содержимом картинки. Поэтому признаки нужно как-то комбинировать.\\n\",\n",
    "\n",
    "__Метод главных компонент__ &mdash; один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков. В рамках метода делается два важных упрощения задачи:\n",
    "1. игнорируется целевая переменная;\n",
    "2. строится линейная комбинация признаков.\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него.\n",
    "\n",
    "PCA ищет линейную комбинацию переменных, чтобы мы могли извлечь максимальную дисперсию из них. По завершении этого процесса он удаляет ее и ищет другую линейную комбинацию (фактор), которая дает объяснение максимальной пропорции оставшейся дисперсии, что приводит к ортогональным факторам. Это повторяется столько раз, чтобы достигнуть целевого числа объясненной дисперсии или числа факторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "Кратко вспомним, что делает этот метод.\n",
    "Пусть $X$ &mdash; матрица объекты-признаки, с нулевым средним каждого признака, а $w$ &mdash; некоторый единичный вектор. Тогда $Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор, который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию вдоль этого направления):\n",
    "$$\\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w$$\n",
    "Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным значением. После этого все пространство проецируется на ортогональное дополнение к вектору $w$ и процесс повторяется.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_synth = np.random.multivariate_normal(\n",
    "    mean=[0, 0],\n",
    "    cov=[[4, 0],\n",
    "         [0, 1]],\n",
    "    size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент. В результате работы PCA из sklearn в dec.components_ будут лежать главные направления (нормированные), а в dec.explained_variance_ — дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_show(dataset):\n",
    "    plt.scatter(*zip(*dataset), alpha=0.5)\n",
    "\n",
    "    dec = PCA()\n",
    "    dec.fit(dataset)\n",
    "    ax = plt.gca()\n",
    "    for comp_ind in range(dec.components_.shape[0]):\n",
    "        component = dec.components_[comp_ind, :]\n",
    "        var = dec.explained_variance_[comp_ind]\n",
    "        start, end = dec.mean_, component * var\n",
    "        ax.arrow(start[0], start[1], end[0], end[1],\n",
    "                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим, что для PCA это ничего не изменит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 6\n",
    "rotate = np.array([\n",
    "        [np.cos(angle), - np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "    ])\n",
    "data_synth_2 = rotate.dot(data_synth.T).T\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки).\n",
    "\n",
    "Вопрос: почему так произошло?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # поиграйте с сидами))\n",
    "\n",
    "data_synth_bad = [\n",
    "    make_circles(n_samples=1000, factor=0.3, noise=0.1)[0]*2,\n",
    "    make_moons(n_samples=1000, noise=0.15)[0]*2,\n",
    "    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n",
    "    np.random.multivariate_normal(\n",
    "        mean=[0, 1.5],\n",
    "        cov=[[2, 1],\n",
    "             [1, 1]],\n",
    "        size=1000),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "rows, cols = 2, 2\n",
    "for i, data in enumerate(data_synth_bad):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    PCA_show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrgGBxPP9WFO"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkxCayiepaHk"
   },
   "source": [
    "Доп.задание : Запустите предыдущий метод с учетом скейлинга. Поменяется ли результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7AaESSvpZXI"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "pipeline = Pipeline(steps=[('sc', scaler), ('pca',pca),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "pcas = \" \".join(f\"{i:.2}\" for i in pca.explained_variance_ratio_)\n",
    "print(f\"Explained {pcas} ({pca.explained_variance_ratio_.sum():.2f}) ratio of variance\")\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, y_hat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt8A-MfMs1hu"
   },
   "source": [
    "Почему важно использование скейлинга перед проведением PCA?\n",
    "Если один объект варьируется больше, чем другие, только из-за их соответствующих масштабов, PCA определит, что такой объект доминирует в направлении главных компонентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlSjGOwPp-d5"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "pipeline = Pipeline(steps=[('pca',pca),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "pcas = \" \".join(f\"{i:.2}\" for i in pca.explained_variance_ratio_)\n",
    "print(f\"Explained {pcas} ({pca.explained_variance_ratio_.sum():.2f}) ratio of variance\")\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, y_hat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc1Fz99tuYV8"
   },
   "source": [
    " Какой метод наиболее страдает от проклятия размерности?\n",
    " KNN.\n",
    "\n",
    " Покажем это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5-m_hbZ4cNF"
   },
   "source": [
    "Рассмотрим набор MNIST - набор рукописных чисел от 0 до 9. Это один из классических наборов для компьютернорго зроения, состоящий из 100 тысяч черно-белых изображений 28х28. Однако матрицы можно и вытянуть в вектор, чтобы предсказывать класс в табличном формате. Тогда мы получим пространство признаков размера 784!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyZuVMWEuFFY"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "X = mnist[\"data\"].astype('float64')\n",
    "X.reset_index()\n",
    "y = mnist[\"target\"].astype('int64')\n",
    "y.reset_index()\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3TMMZpB4-Wi"
   },
   "source": [
    "Так выглядит набор данных в табличном виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzal5vP3Pvz"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK0ItAvi5Cmg"
   },
   "source": [
    "Рассмотрим случайный семпл в данных (преобразовав обратно в матрицу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKJTMUxw2mDz"
   },
   "outputs": [],
   "source": [
    "random_digit = X.loc[10,:].values\n",
    "\n",
    "random_digit_image = random_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(random_digit_image, cmap = 'gray', interpolation=\"nearest\")\n",
    "#plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5uLQXRM5Joj"
   },
   "source": [
    "Отнормируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYHAcyeR2rMV"
   },
   "outputs": [],
   "source": [
    "X /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbAPTh3I2ruG"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCjv5I1i5NeM"
   },
   "source": [
    "Заодно с обучением модели вспомним о поиске гиперпараметров. Что это такое?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oLX7reE8Nar"
   },
   "source": [
    "Так как данных очень много, равномерно засемплим 1000 точек. В общем случае это плохая практика. Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZgmTcG58Nzs"
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X.index, 10000, replace=False)\n",
    "X_rus = X.loc[random_indices]\n",
    "y_rus = y.loc[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxzrVSGw2sMi"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NPp7UKt2stX"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [3, 4], 'p': [2], 'weights': [\"distance\"]}\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_knn = # YOUR CODE: get best params\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TrSQfcM75g4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##########\n",
    "# YOUR CODE \n",
    "# fit knn with best params\n",
    "#########\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "\n",
    "train_accuracy_knn = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_zj69cH9dr-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_train_pred = cross_val_predict(knn, X_train, y_train, cv=5)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "\n",
    "test_accuracy_knn = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTraining Accuracy: \", test_accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUMrtWsi9nVb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The accuracy of the model\n",
    "test_accuracy_knn = knn.score(X_test, y_test)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy_knn)\n",
    "\n",
    "\n",
    "# No. of Correct Predictions\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "print(\"\\nNo. of correct predictions (Test): %d/%d\" % (np.sum(y_test_predicted == y_test), len(y_test)))\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d4fOpdZ92UL"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_test_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N6phuYv-TPh"
   },
   "source": [
    "Попробуем уменьшить влияние проклятия размерности с помощью уменьшения размерности, использовав метод PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDHPls-d93jl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "print(\"Number of Principle Components: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBKiZN_W-CrS"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# YOUR CODE\n",
    "# Transform x train and test\n",
    "# X_train_pca = \n",
    "# X_test_pca =\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIIDgyc5-Eo2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=4, p=2, weights=\"distance\")\n",
    "\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "y_test_predicted_pca_knn = knn_pca.predict(X_test_pca)\n",
    "print(\"KNN (PCA): Test Accuracy: \", accuracy_score(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK5rCzrt-JzJ"
   },
   "source": [
    "Мы видим, что после уменьшения размерности мы можем снизить стоимость расчета расстояния в K-NN, что приводит к значительному сокращению времени обучения. При этом мы не потеряли в точности, а даже выиграли!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXt5fDBz_SYx"
   },
   "source": [
    "Для сравнения времени обучения рассмотрим метод, построенный не на расстоянии - случайный лес. Эта модель будет страдать от проклятия размерности в терминах времени меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZA2XP_o-M5Q"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH71bPpS_j26"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test_pca)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuDLiENxCz70"
   },
   "source": [
    "Как видно, обучение произошло гораздо быстрее, при этом точность алгоритма даже выше, чем у KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проиллюстрируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(filename, size=64):\n",
    "    with Image.open(filename) as img:\n",
    "        # Convert to grayscale and resize\n",
    "        img = img.convert('L').resize((size, size))\n",
    "        \n",
    "        # Convert to numpy array with channel dimension\n",
    "        arr = np.array(img, dtype=np.uint8)[..., np.newaxis]\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим небольшой датасет с картинками котов. Для простоты переведем ихз в черно-белую гамму и сделаем их размера 64 на 64. ПОсмотрим, что получится. При этом выведем еще и \"среднего кота\". Потом нам это понадобится для PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "random.seed(1234)\n",
    "cats_path = os.path.join(data_path, \"base_cats\")\n",
    "image_shape = (64, 64)\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "\n",
    "all_files = [f for f in os.listdir(cats_path ) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "processed = []\n",
    "for filename in random.sample(all_files, 120):\n",
    "    processed.append(process_image(os.path.join(cats_path, filename)))\n",
    "    \n",
    "sample = random.sample(processed, n_samples)\n",
    "cats_data = np.array(processed, dtype=np.float32)\n",
    "\n",
    "mean_cat = cats_data.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for i in range(n_samples - 1):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(cats_data[i, :].reshape(image_shape), interpolation='none',\n",
    "               cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "plt.subplot(rows, cols, n_samples)\n",
    "plt.imshow(mean_cat.reshape(image_shape), interpolation='none',\n",
    "           cmap='gray')\n",
    "plt.xticks(())\n",
    "_ = plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к ним PCA. Найдем первые главные компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = PCA()\n",
    "cats_data -= mean_cat  # отнормировали данные к нулевому среднему\n",
    "model_pca.fit(cats_data.reshape(-1, 64*64))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(model_pca.components_[i, :].reshape(image_shape), interpolation='none', cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились забавные призраки! Обратите внимание, что это не изображение котов, а именно компоненты - или собственные векторы $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VqtFGGjec-ZS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
