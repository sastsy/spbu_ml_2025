{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Разминка*: Всегда ли решение задачи линейной регрессии единственно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Вопрос*: А если все же линейной зависимости нет? Подсказка: решение задачи регрессии имеет вид. $(X^TX)^{-1}X^Ty$.\n",
    " \n",
    "\n",
    "*Вопрос*: К чему это приведет?\n",
    "\n",
    "\n",
    "*Вопрос:* Какие еще последствия у мультиколлинеарности?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import (\n",
    "    HuberRegressor,\n",
    "    LinearRegression,\n",
    "    RANSACRegressor,\n",
    "    TheilSenRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import load_diabetes, fetch_openml,load_iris,fetch_california_housing\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переобучение\n",
    "В машинном обучении часто говорят об *обобщающей способности модели*, то есть о способности модели работать на новых тестовых данных хорошо. Если модель будет идеально предсказывать выборку, на которой она обучалась, но при этом просто ее запомнит, не \\\"вытащив\\\" из данных никакой закономерности, от нее будет мало толку. Такую модель называют *переобученной*: она слишком подстроилась под обучающие примеры, не выявив никакой полезной закономерности, которая позволила бы ей совершать хорошие предсказания на данных, которые она ранее не видела.\\n\",\n",
    "\n",
    "Рассмотрим следующий пример, на котором будет хорошо видно, что значит переобучение модели. Для этого нам понадобится сгенерировать синтетические данные. Рассмотрим зависимость $y = cos(1.5\\pi x)$, \n",
    " $y $ — целевая переменная, а $x$ - объект (в нашем случае это число от 0 до 1). В жизни мы наблюдаем какое-то конечное количество пар объект-таргет, поэтому смоделируем это, взяв 30 случайных точек  $x_i, y_i$  в отрезке $[0, 1]$.\n",
    "\n",
    "В реальной жизни целевая переменная может быть зашумленной, смоделируем это, зашумив значение функции нормальным шумом: $\\tilde{y}_i = y(x_i) + \\mathcal{N}(0, 0.01)$.\n",
    "\n",
    "Попытаемся обучить три разных линейных модели: признаки для первой - $\\{x\\}$, для второй - $\\{x, x^2, x^3, x^4\\}$, для третьей - $\\{x, \\dots, x^{20}\\}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(36)\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.cos(1.5 * np.pi * x)\n",
    "\n",
    "x_objects = np.random.uniform(0, 1, size=30)\n",
    "y_objects = np.cos(1.5 * np.pi * x_objects) + np.random.normal(scale=0.1, size=x_objects.shape)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "fig, ax = plt.subplots(figsize=(16, 4), ncols=3)\n",
    "for i, degree in enumerate([1, 4, 20]):\n",
    "    X_objects = PolynomialFeatures(degree, include_bias=False).fit_transform(x_objects[:, None])\n",
    "    X = # YOUR CODE: Transform x (to get nice line of prediction)\n",
    "    model = LinearRegression().fit(X_objects, y_objects)\n",
    "    y_pred = model.predict(X)\n",
    "    ax[i].plot(x, y, label=\"Real function\")\n",
    "    ax[i].scatter(x_objects, y_objects, label=\"Data\")\n",
    "    ax[i].plot(x, y_pred, label=\"Prediction\")\n",
    "    if i == 0:\n",
    "        axs[i].legend()\n",
    "    ax[i].set_title(\"Degree = %d\" % degree)\n",
    "    ax[i].set_xlabel(\"$x$\")\n",
    "    ax[i].set_ylabel(\"$f(x)$\")\n",
    "    ax[i].set_ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: Почему первая модель получилась плохой, а третья переобучилась?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать переобучения, модель регуляризуют. На этом занятии мы как раз рассмотрим различные методы регуляризации линейных моделей.\n",
    "\n",
    "Но, прежде, чем начать, создадим свою версию линейной модели, которую можно инициализировать готовыми весами и не обучать на данных. Зачем нам это понадобится? Узнаем чуть позже.\n",
    "Итак, создайте класс линейной модели, которую можно будет инициализировать заданными коэффициентами. Она должна реализовывать интерфейс модели sklearn. Можете отнаследоваться от стандартной линейной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPredictionModel(LinearRegression):\n",
    "    def __init__(self, coef=None, intercept=None):\n",
    "        if coef is not None:\n",
    "            coef = # make a numpy array from coefficients\n",
    "            if intercept is None:\n",
    "                intercept = np.zeros(coef.shape[0])\n",
    "            else:\n",
    "                intercept = np.array(intercept)\n",
    "            assert coef.shape[0] == intercept.shape[0]\n",
    "        else:\n",
    "            if intercept is not None:\n",
    "                raise ValueError(\"Provide coef only or both coef and intercept\")\n",
    "        self.intercept_ = intercept\n",
    "        self.coef_ = coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для первых экспериментов также создадим случайный набор данных, где x имеет 2 признака. Можете обучить свою или стандартную модель на этих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5,2)\n",
    "real_alpha = 1.5\n",
    "real_beta = 2.\n",
    "y = real_alpha * x[:, 0] + real_beta * x[:, 1] + np.random.normal(loc=0., scale=0.2)\n",
    "model = LinearRegression().fit(x, y)\n",
    "print(f\"coefficients: {model.coef_}, intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая будет сохранять значение ошибки для разных значений параметров. Функция должна возвращать датафрейм со столбцами `[\"a\", \"b\", \"error\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors_table(\n",
    "    alphas: list[float] | NDArray, betas: list[float] | NDArray, x: NDArray, y: NDArray\n",
    ") -> pd.DataFrame:\n",
    "    result = pd.DataFrame(columns=[\"a\", \"b\", \"error\"])\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            res = {\"a\": alpha, \"b\": beta}\n",
    "            model = LinearPredictionModel(coef=[[alpha, beta]], intercept=[[0]])\n",
    "            res[\"error\"] = mean_squared_error(model.predict(x), y)\n",
    "            result = pd.concat([result, pd.DataFrame(res, index=[0])], axis=0, ignore_index=True)\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим сетку параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(real_alpha - 2, real_alpha + 2, 101)\n",
    "betas = np.linspace(real_beta - 2, real_beta + 2, 101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_errors_table(alphas, betas, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте поверхность функции потерь в 3d. Для отрисовки удобнее всего воспользовать функцией `plot_trisurf`. Для создания графика можно воспользоваться `fig.add_subplot(subplot_pos (int: n_hor|n_vert|position), projection='3d')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# Один способ нарисовать 3d график\n",
    "x1 = np.linspace(df['a'].min(), df['a'].max(), len(df['a'].unique()))\n",
    "y1 = np.linspace(df['b'].min(), df['b'].max(), len(df['b'].unique()))\n",
    "x2, y2 = np.meshgrid(x1, y1) \n",
    "\n",
    "z2 = griddata((df['a'], df['b']), df['error'], (x2, y2), method='cubic')\n",
    "ax.plot_trisurf(df[\"a\"], df[\"b\"], df[\"error\"], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно нарисовать линии уровня для этой поверхности. Можно воспользоваться функцией `ax.tricontour`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "CS = ax.tricontour(df['a'], df['b'], df['error'], levels=15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь можно переходить к вопросам регуляризации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо исходной задачи можно решать другую, с учетом регуляризации:\n",
    "\\begin{equation}\n",
    "\\min_w L(f, X, y) = \\min_w(|X w - y|_2^2 + \\lambda |w|^k_k ),\n",
    "\\end{equation}\n",
    "где \n",
    "\\begin{equation}\n",
    "|w|^2_2 = w^2_1 + \\ldots + w^2_n\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "|w|_1^1 = \\vert w_1 \\vert + \\ldots + \\vert w_n \\vert\n",
    "\\end{equation}\n",
    "\n",
    "Коэффициент $\\lambda$ - коэффициент регуляризации, его часто подбирают по логарифмической шкале.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Нужно ли регуляризовать отступ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение новой задачи (L2) имеет вид \n",
    "\\begin{equation} \n",
    "w = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "\\end{equation}\n",
    "В случае плохо обусловленной матрицы можно сразу увидеть эффект от регуляризации. Какой?\n",
    "Градиент же имеет вид\n",
    "\\begin{equation}\n",
    "\\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\\lambda w\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Какой вариант градиентного спуска правильный для батча размера N:\n",
    "\\begin{equation}\n",
    "(a) w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{2\\alpha\\lambda}N w_i,\\quad i=1,\\ldots,D\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "(b) w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\alpha\\lambda w_i,\\quad i=1,\\ldots,D\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "(c) w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\lambda N w_i,\\quad i=1,\\ldots D\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Как выглядит градиент в случае L1- регуляризации? \n",
    "\n",
    "Вопрос: А как быть в точке, где градиента не существует?\n",
    "\n",
    "Вопрос: Можно ли применить регуляризацию для другой функции потерь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем геометрически отрисовать, как выглядят составляющие функции потерь после регуляризации. Для начала реализуем функции рассчета нормы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def l1_norm(vector):\n",
    "    return # Make a l1 norm regularizer for vector\n",
    "\n",
    "def l2_norm(vector):\n",
    "    return # Make a l2 norm regularizer for vector\n",
    "\n",
    "def p_norm(vector, p: float=1.):\n",
    "    return  # Make a lp norm regularizer for vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим пространство параметров, как и ранее, но построим таблицу норм весов вместо ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace( - 2, real_alpha + 2, 101)\n",
    "betas = np.linspace( - 2, real_beta + 2, 101)\n",
    "\n",
    "def get_norm_plot(alphas, betas, norm_fn):\n",
    "    result = pd.DataFrame(columns=[\"a\", \"b\", \"norm\"])\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            res = {\"a\": alpha, \"b\": beta}\n",
    "            res[\"norm\"] = norm_fn(np.array([alpha, beta]))\n",
    "            result = pd.concat([result, pd.DataFrame(res, index=[0])], axis=0, ignore_index=True)\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = get_norm_plot(alphas, betas, l1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax_1 = fig.add_subplot(121, projection='3d')\n",
    "ax_2 = fig.add_subplot(122)\n",
    "\n",
    "# Interpolate unstructured D-dimensional data.\n",
    "ax_1.plot_trisurf(df_norm[\"a\"], df_norm[\"b\"], df_norm[\"norm\"], alpha=0.5)\n",
    "ax_2.tricontour(df_norm['a'], df_norm['b'], df_norm['norm'], levels=15 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = get_norm_plot(alphas, betas, l2_norm)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax_1 = fig.add_subplot(121, projection='3d')\n",
    "ax_2 = fig.add_subplot(122)\n",
    "\n",
    "# Interpolate unstructured D-dimensional data.\n",
    "ax_1.plot_trisurf(df_norm[\"a\"], df_norm[\"b\"], df_norm[\"norm\"], alpha=0.5)\n",
    "ax_2.tricontour(df_norm['a'], df_norm['b'], df_norm['norm'], levels=15 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Что будет с поверхностью, если строить график для регулязированной функции ошибки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы проверить свой ответ, построим его. Для этого реализуйте функцию для рассчета новой ошибки (с функцией регуляризации как параметром)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_plot(alphas, betas, norm_fn, gamma, x, y):\n",
    "    result = pd.DataFrame(columns=[\"a\", \"b\", \"error\"])\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            res = {\"a\": alpha, \"b\": beta}\n",
    "            model = # Make a linear model with given coefficients. Use intercept 0 \n",
    "            res[\"error\"] = mean_squared_error(model.predict(x), y) + gamma * norm_fn(np.array([alpha, beta]))\n",
    "            result = pd.concat([result, pd.DataFrame(res, index=[0])], axis=0, ignore_index=True)\n",
    "    return result\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace( - 2, real_alpha + 2, 101)\n",
    "betas = np.linspace( - 2, real_beta + 2, 101)\n",
    "df_norm = get_error_plot(alphas, betas, l1_norm, 4., x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax_1 = fig.add_subplot(121, projection='3d')\n",
    "ax_2 = fig.add_subplot(122)\n",
    "\n",
    "ax_1.plot_trisurf(df_norm[\"a\"], df_norm[\"b\"], df_norm[\"error\"], alpha=0.5)\n",
    "ax_2.tricontour(df_norm['a'], df_norm['b'], df_norm['error'], levels=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace( - 2, real_alpha + 2, 101)\n",
    "betas = np.linspace( - 2, real_beta + 2, 101)\n",
    "df_norm = get_error_plot(alphas, betas, l2_norm, 4., x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax_1 = fig.add_subplot(121, projection='3d')\n",
    "ax_2 = fig.add_subplot(122)\n",
    "\n",
    "ax_1.plot_trisurf(df_norm[\"a\"], df_norm[\"b\"], df_norm[\"error\"], alpha=0.5)\n",
    "ax_2.tricontour(df_norm['a'], df_norm['b'], df_norm['error'], levels=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 1**: Почему большие веса в линейной модели — плохо?\n",
    "\n",
    "**Вопрос 2**: Почему регуляризовать $w_0$\n",
    " — плохая идея?\n",
    " \n",
    "**Вопрос 3**: На что влияет коэффициент $\\lambda$? Что будет происходить с моделью, если его\n",
    " начать уменьшать? Что будет, если его сделать слишком большим?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спарсность при использовании L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У L1-регрессии есть еще одно замечательное свойство. Оно заключается в том, что оптимальное решение в случае задачи с ограничениями находится в точке пересечения с основной поверхностью. Это будет происходить в углах ромба, где один из весов занулен. В результате, при увеличении размерности, у признаков, не оказывающих влияния на ответы, веса постепенно зануляются.\n",
    "Таким образом, этот вид регрессии тоже можно использовать для выбора признаков.\n",
    "\n",
    "*Вопрос:* Может быть задача, при которой спарсность не будет наблюдаться?\n",
    "\n",
    "Для иллютрации рассмотрим уже знакомый датасет по классификации цифр. Однако, для нужд регрессии сделаем таргет, равный \"яркости\" написания - суммы всех значений пикселей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "X = mnist[\"data\"].astype('float64')\n",
    "X.reset_index()\n",
    "y_c = mnist[\"target\"].astype('int64')\n",
    "y_c.reset_index()\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.sum(axis=1) + X.iloc[:,0:20].sum(axis=1) * 2 + X.iloc[:,100:400].sum(axis=1) * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X.index, 10000, replace=False)\n",
    "X_rus = X.loc[random_indices]\n",
    "y_rus = y.loc[random_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.coef_[3], model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=1.)\n",
    "# YOUR CODE: fit lasso\n",
    "print(lasso.coef_[3], lasso.intercept_)\n",
    "sns.kdeplot(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличим значение регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=4.)\n",
    "# YOUR CODE: fit lasso\n",
    "print(lasso.coef_[3], lasso.intercept_)\n",
    "sns.kdeplot(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=20.)\n",
    "# YOUR CODE: fit lasso\n",
    "print(lasso.coef_[3], lasso.intercept_)\n",
    "sns.kdeplot(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, чем больше параметр регуляризации, тем больше весов уходит в ноль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.coef_[3], model.intercept_)\n",
    "sns.kdeplot(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, будем ли мы наблюдать такой же эффект в случае Ridge регрессии. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same eexperiment with alpha=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same eexperiment with alpha=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в данном случае это не наблюдается. Кстати, это можно понять и по геометрическому представлению L2-регуляризации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой из видов регуляризации выбрать? \n",
    "Часто решение зависит от задачи.\n",
    "\n",
    "Так, L2-регрессия больше внимания уделяет большим весам, а вот относительно маленькие будут в низком приоритете. Почему?\n",
    "\n",
    "Также спарсность - очень полезное свойство. Однако иногда L1 регрессия проигрывает L1. Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того:\n",
    "- В случае p>n L1 выбирает не более n переменных до насыщения из-за природы задачи выпуклой оптимизации.\n",
    "- Если существует группа переменных, среди которых попарные корреляции очень высоки, то L1 стремится выбрать только одну переменную из группы, и ей все равно, какую именно\n",
    "- Для обычных n>p ситуаций, если существуют высокие корреляции между предикторами,эмпирически замечено, что в предсказательной производительности L2 доминирует над L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "Эта модель была предложена как способ исправить проблемы первых двух пунктов, но при этом достичь лучших рещультатов, чем Lasso, в целом.\n",
    "Она устроена на удивление просто, как комбинация первых двух, и практически всегда будет хорошим решением. Единственная проблема в том, что в ее случае приходится тюнить сразу два параметра:\n",
    "\\begin{equation}\n",
    "\\min_w L(f, X, y) = \\min_w(|X w - y|_2^2 + \\lambda_1 |w|^2_2 + \\lambda_2 |w|_1),\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elastic Net\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Make the same eexperiment as for ridge and lasso with alpha=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае мы наблюдаем смешанный эффект. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Протестируйте разные методы регуляризации на наборе данных из начала занятия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# \n",
    "# YOUR CODE\n",
    "# 1) make a regression data\n",
    "# 1) make a linear model with degree = 20\n",
    "# 2) Try lasso, ridge and elastic net regression with at least two or three different alphas\n",
    "# 3) Draw the plot: on one axis (three plots in one) show the sparsity level for different lambdas of each regularizer\n",
    "# 4) Draw the plot (or make table): show the error for different lambdas of each regularizer\n",
    "#\n",
    "###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация\n",
    "Конечно, для классификации также можно использовать регуляризацию. Рассмотрим логистическую регрессию как последовательность из линейной модели и сигмоиды. Напишите функцию для получения ошибки, как раньше, но используя новую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+(np.exp((-z))))\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_error_plot_sigmoid(alphas, betas, norm_fn, gamma, x, y):\n",
    "    result = pd.DataFrame(columns=[\"a\", \"b\", \"error\"])\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            res = {\"a\": alpha, \"b\": beta}\n",
    "            model = LinearPredictionModel(coef=[[alpha, beta]], intercept=[[0]])\n",
    "            res[\"error\"] = mean_squared_error(y, sigmoid(model.predict(x))) + gamma * norm_fn(np.array([alpha, beta]))\n",
    "            result = pd.concat([result, pd.DataFrame(res, index=[0])], axis=0, ignore_index=True)\n",
    "    return result\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = np.random.randn(30,2)\n",
    "real_alpha = 1.5\n",
    "real_beta = 2.\n",
    "line = real_alpha * x[:, 0] + real_beta * x[:, 1] \n",
    "y = (line + np.random.normal(loc=0, scale=0.5, size=x.shape[0]) > line).astype(int)\n",
    "model = LogisticRegression().fit(x, y)\n",
    "model.coef_\n",
    "np.sum((model.predict(x) > 0.5) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим поверхность без нормализации. На самом деле, за счет того, что сигмоида - не выпуклая функция, поверхность также перестает быть выпуклой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace( - 3, real_alpha + 3, 101)\n",
    "betas = np.linspace( - 3, real_beta + 3, 101)\n",
    "df_norm = get_error_plot_sigmoid(alphas, betas, l1_norm, 0.0, x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax_1 = fig.add_subplot(121, projection='3d')\n",
    "ax_2 = fig.add_subplot(122)\n",
    "ax_1.plot_trisurf(df_norm[\"a\"], df_norm[\"b\"], df_norm[\"error\"], alpha=0.5)\n",
    "ax_2.tricontour(df_norm['a'], df_norm['b'], df_norm['error'], levels=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: К чему это приводит?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним также разные регрессоры для датасета с цифрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X.index, 10000, replace=False)\n",
    "X_rus = X.loc[random_indices]\n",
    "y_rus = y_c.loc[random_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(penalty=None, max_iter=30)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train accuracy is: \", model.score(X_train / 255., y_train))\n",
    "print(\"Test accuracy is: \", model.score(X_test / 255., y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(model.coef_.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(penalty=\"l2\", alpha=0.001, max_iter=30)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train accuracy is: \", model.score(X_train / 255, y_train))\n",
    "print(\"Test accuracy is: \", model.score(X_test / 255, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(penalty=\"l1\", alpha=0.1, max_iter=30)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train accuracy is: \", model.score(X_train, y_train))\n",
    "print(\"Test accuracy is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(penalty=\"elasticnet\", alpha=0.01, max_iter=30)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train accuracy is: \", model.score(X_train, y_train))\n",
    "print(\"Test accuracy is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, что будет, если использовать функцию квадратичной ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(penalty=\"l1\", alpha=0.01, max_iter=200, loss=\"squared_error\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train accuracy is: \", model.score(X_train, y_train))\n",
    "print(\"Test accuracy is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная секция. Lars\n",
    "Least angle regrssion (LARS) — это алгоритм регрессии для многомерных данных, разработанный Брэдли Эфроном, Тревором Хасти, Иэном Джонстоном и Робертом Тибширани (он же автор Lasso). LARS немного похож на PCA. На каждом этапе он находит признак, наиболее коррелирующий с целевой переменной. Когда существует несколько признаков, имеющих одинаковую корреляцию, вместо продолжения вдоль одного признака, процесс продолжается в направлении, равноугольном между ними.\n",
    "\n",
    "- Он численно эффективен при p >> n.\n",
    "- В вычислительном отношении он так же быстр, как и последовательный выбор признаков, и имеет тот же порядок сложности, что и обычный метод наименьших квадратов.\n",
    "- Он создает полный кусочно-линейный путь решения, который полезен при перекрестной проверке или аналогичных попытках настройки модели.\n",
    "- Если два признака почти одинаково коррелируют с целью, то их коэффициенты должны увеличиваться примерно с одинаковой скоростью. При этом алгоритм является более стабильным, чем прямой выбор признаков.\n",
    "Однако, поскольку LARS основан на итеративной переработке ошибок, он может оказаться особенно чувствительным к влиянию шума. Эта проблема подробно обсуждается Вейсбергом в дискуссионном разделе книги Эфрона и др. (2004).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path, LassoLars, Lars\n",
    "from sklearn import metrics\n",
    "model = LassoLars(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train r2 is: \", model.score(X_train, y_train))\n",
    "print(\"Test r2  is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path, LassoLars, Lars\n",
    "from sklearn import metrics\n",
    "model = LassoLars(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train r2 is: \", model.score(X_train, y_train))\n",
    "print(\"Test r2  is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как понять, какие признаки в результате важны, а какие нет?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: а какие признаки отберут другие методы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train r2 is: \", model.score(X_train, y_train))\n",
    "print(\"Test r2  is: \", model.score(X_test, y_test))\n",
    "sns.kdeplot(model.coef_.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью Lars можно также посмотреть на \"пути\" переменных при уменьшении степени регуляризации. Так можно выбрать, например, l1 регуляризацию с необходимым уровнем спарсности/точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing regularization path using the LARS ...\")\n",
    "_, _, coefs = lars_path(X_train, y_train, method=\"lar\", verbose=True)\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "#plt.vlines(xx, ymin, ymax, linestyle=\"dashed\")\n",
    "plt.xlabel(\"|coef| / max|coef|\")\n",
    "plt.ylabel(\"Coefficients\")\n",
    "plt.title(\"LASSO Path\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
